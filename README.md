# LLM-lawyer
Нейросетевая модель для юристов

## Описание проекта
Система на Python, которая локально анализирует корпус текстов судебных решений, обучает нейросеть и генерирует мотивировочную часть (аргументацию) в виде связного текста на основе фактических обстоятельств дела.

### Особенности
- Полностью локальная работа без обращения к внешним API
- Автоматический анализ фактических обстоятельств дела
- Обучение собственной нейросети на корпусе судебных решений
- Генерация связной мотивировочной части (аргументации)
- Минимальный графический интерфейс для ввода фактов и получения результата

## Поэтапный план выполнения
- [x] **Шаг 0: Инициализация проекта и планирование**
  - [x] Создание репозитория
  - [x] Подготовка плана разработки
  - [x] Документирование требований

- [x] **Шаг 1: Подготовка окружения**
  - [x] Создать структуру папок (data/raw, data/processed, models, src, gui)
  - [x] Создать и активировать виртуальное окружение
  - [x] Установить базовые библиотеки (torch, python-dotenv, pdfminer.six и др.)
  - [x] Создать файл requirements.txt

- [x] **Шаг 2: Загрузка и хранение "сырых" данных**
  - [x] Скопировать PDF-файлы с решениями суда в data/raw/
  - [x] Проверить корректность файлов и их расширений
  - [x] Подготовить список файлов внутри data/raw/

- [x] **Шаг 3: Препроцессинг: конвертация PDF → текст**
  - [x] Реализовать функцию convert_pdf_to_text() в src/preprocess.py
  - [x] Создать скрипт для обработки всех PDF из data/raw/
  - [x] Сохранить результаты в data/processed/ в виде .txt файлов
  - [x] Исправить проблемы с кодировкой для бюллетеней

- [x] **Шаг 4: Очистка и разделение текстов на разделы**
  - [x] Добавить функцию split_into_sections() для выделения частей документа
  - [x] Разделить тексты на секции: факты, мотивировка, выводы
  - [x] Сохранить структурированные данные в формате JSON

- [x] **Шаг 5: Формирование обучающего датасета**
  - [x] Создать src/build_dataset.py для формирования пар "факты → мотивировка"
  - [x] Сохранить датасет в формате JSONL
  - [x] Проверить корректность и полноту датасета

- [ ] **Шаг 6: Реализация компонента обучения (train)**
  - [ ] Создать класс LegalDataset для работы с данными
  - [ ] Реализовать архитектуру нейросети (Transformer или Seq2Seq LSTM)
  - [ ] Написать цикл обучения с сохранением лучшей модели
  - [ ] Добавить параметризацию через аргументы командной строки

- [ ] **Шаг 7: Проверка модели на "сухом" примере (валидация инференса)**
  - [ ] Реализовать функции load_model() и generate() в src/inference.py
  - [ ] Создать тестовый пример для проверки генерации
  - [ ] Проверить качество и связность генерируемого текста

- [ ] **Шаг 8: Создание простейшего GUI (app.py)**
  - [ ] Создать окно с полями для ввода фактов и вывода мотивировки
  - [ ] Добавить кнопки для генерации и копирования в буфер
  - [ ] Реализовать взаимодействие с моделью через интерфейс

- [ ] **Шаг 9: Проверка всего "энд-ту-энд" (запуск от "PDF→GUI")**
  - [ ] Проверить полный цикл работы от загрузки PDF до генерации текста
  - [ ] Убедиться в отсутствии ошибок и корректности путей
  - [ ] Протестировать интеграцию всех компонентов

- [ ] **Шаг 10: Базовый мониторинг и логирование ошибок**
  - [ ] Добавить логирование во все ключевые компоненты
  - [ ] Реализовать информативные сообщения о статусе выполнения
  - [ ] Обеспечить корректную обработку исключений

- [ ] **Шаг 11: Документирование**
  - [ ] Дополнить README.md подробными инструкциями
  - [ ] Описать структуру проекта, шаги установки и запуска
  - [ ] Добавить примеры команд и советы по тестированию

- [ ] **Шаг 12: Базовое тестирование стабильности**
  - [ ] Создать контрольный поднабор данных
  - [ ] Проверить систему на стабильность работы
  - [ ] Оптимизировать при необходимости для снижения времени обработки

- [ ] **Шаг 13: Финальные замечания и рекомендации**
  - [ ] Проверить качество данных и объем обучающей выборки
  - [ ] Рассмотреть возможные улучшения системы
  - [ ] Оценить производительность и дать рекомендации по оптимизации

## Запуск проекта
После завершения разработки, запуск системы будет выполняться следующей последовательностью команд:

```bash
# Предобработка PDF документов
python src/preprocess.py
python src/preprocess.py --input-dir data/raw --output-dir data/processed

# Создание обучающего датасета
python src/build_dataset.py data/processed/ data/train_dataset.jsonl

# Обучение модели
python src/train.py --epochs 10 --batch-size 4

# Запуск графического интерфейса
python gui/app.py
```

## Требования и ограничения
- Система работает полностью локально без обращения к внешним API
- Для качественного результата требуется достаточный корпус текстов (рекомендуется 500-1000 документов)
- Графический интерфейс минималистичен, но функционален на всех основных ОС
- Для обучения модели рекомендуется наличие GPU