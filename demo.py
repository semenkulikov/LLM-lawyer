#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–æ–∫–∞–∑–∞ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –º–æ—Ç–∏–≤–∏—Ä–æ–≤–æ—á–Ω–æ–π —á–∞—Å—Ç–∏
—Ä–µ—à–µ–Ω–∏—è —Å—É–¥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–æ–¥–µ–ª–∏ QVikhr
"""

import os
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º src –≤ –ø—É—Ç—å –¥–ª—è –∏–º–ø–æ—Ä—Ç–∞ –º–æ–¥—É–ª–µ–π
sys.path.append(str(Path(__file__).parent / "src"))

from inference import load_model, generate
from loguru import logger

def demo_qvikhr():
    """
    –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç—ã –º–æ–¥–µ–ª–∏ QVikhr –Ω–∞ –ø—Ä–∏–º–µ—Ä–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–µ–ª–∞
    """
    print("="*80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –°–ò–°–¢–ï–ú–´ –ì–ï–ù–ï–†–ê–¶–ò–ò –ú–û–¢–ò–í–ò–†–û–í–û–ß–ù–û–ô –ß–ê–°–¢–ò")
    print("–ú–æ–¥–µ–ª—å: QVikhr (–¥–æ–æ–±—É—á–µ–Ω–Ω–∞—è –Ω–∞ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö)")
    print("="*80)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
    model_path = "models/legal_model"
    if not os.path.exists(model_path):
        print(f"‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –ø–æ –ø—É—Ç–∏: {model_path}")
        print("–£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –º–æ–¥–µ–ª—å QVikhr –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é models/legal_model")
        return
    
    try:
        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        print("üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ QVikhr...")
        model, tokenizer = load_model(model_path)
        print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        
        # –ü—Ä–∏–º–µ—Ä —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–µ–ª–∞
        facts = """
        –ò—Å—Ç–µ—Ü –ò–≤–∞–Ω–æ–≤ –ò.–ò. –æ–±—Ä–∞—Ç–∏–ª—Å—è –≤ —Å—É–¥ —Å –∏—Å–∫–æ–º –∫ –æ—Ç–≤–µ—Ç—á–∏–∫—É –ü–µ—Ç—Ä–æ–≤—É –ü.–ü. –æ –≤–∑—ã—Å–∫–∞–Ω–∏–∏ –∑–∞–¥–æ–ª–∂–µ–Ω–Ω–æ—Å—Ç–∏ –ø–æ –¥–æ–≥–æ–≤–æ—Ä—É –∑–∞–π–º–∞ –≤ —Ä–∞–∑–º–µ—Ä–µ 150 000 —Ä—É–±–ª–µ–π, –∞ —Ç–∞–∫–∂–µ –ø—Ä–æ—Ü–µ–Ω—Ç–æ–≤ –∑–∞ –ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —á—É–∂–∏–º–∏ –¥–µ–Ω–µ–∂–Ω—ã–º–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ –≤ —Ä–∞–∑–º–µ—Ä–µ 15 000 —Ä—É–±–ª–µ–π.
        
        –í –æ–±–æ—Å–Ω–æ–≤–∞–Ω–∏–µ –∏—Å–∫–∞ –∏—Å—Ç–µ—Ü —É–∫–∞–∑–∞–ª, —á—Ç–æ 15 —è–Ω–≤–∞—Ä—è 2023 –≥–æ–¥–∞ –º–µ–∂–¥—É —Å—Ç–æ—Ä–æ–Ω–∞–º–∏ –±—ã–ª –∑–∞–∫–ª—é—á–µ–Ω –¥–æ–≥–æ–≤–æ—Ä –∑–∞–π–º–∞, –ø–æ —É—Å–ª–æ–≤–∏—è–º –∫–æ—Ç–æ—Ä–æ–≥–æ –∏—Å—Ç–µ—Ü –ø–µ—Ä–µ–¥–∞–ª –æ—Ç–≤–µ—Ç—á–∏–∫—É –¥–µ–Ω–µ–∂–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ –≤ —Ä–∞–∑–º–µ—Ä–µ 150 000 —Ä—É–±–ª–µ–π —Å—Ä–æ–∫–æ–º –Ω–∞ 3 –º–µ—Å—è—Ü–∞. –°—Ä–æ–∫ –≤–æ–∑–≤—Ä–∞—Ç–∞ –∑–∞–π–º–∞ –∏—Å—Ç–µ–∫ 15 –∞–ø—Ä–µ–ª—è 2023 –≥–æ–¥–∞, –æ–¥–Ω–∞–∫–æ –æ—Ç–≤–µ—Ç—á–∏–∫ –Ω–µ –≤–æ–∑–≤—Ä–∞—Ç–∏–ª –∑–∞–µ–º–Ω—ã–µ —Å—Ä–µ–¥—Å—Ç–≤–∞ –≤ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–π —Å—Ä–æ–∫.
        
        –û—Ç–≤–µ—Ç—á–∏–∫ –≤ —Å—É–¥–µ–±–Ω–æ–µ –∑–∞—Å–µ–¥–∞–Ω–∏–µ –Ω–µ —è–≤–∏–ª—Å—è, –æ –≤—Ä–µ–º–µ–Ω–∏ –∏ –º–µ—Å—Ç–µ —Å—É–¥–µ–±–Ω–æ–≥–æ –∑–∞—Å–µ–¥–∞–Ω–∏—è –±—ã–ª –∏–∑–≤–µ—â–µ–Ω –Ω–∞–¥–ª–µ–∂–∞—â–∏–º –æ–±—Ä–∞–∑–æ–º. –í –ø–∏—Å—å–º–µ–Ω–Ω—ã—Ö –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è—Ö –æ—Ç–≤–µ—Ç—á–∏–∫ –Ω–µ –æ—Ç—Ä–∏—Ü–∞–ª —Ñ–∞–∫—Ç –ø–æ–ª—É—á–µ–Ω–∏—è –∑–∞–π–º–∞, –Ω–æ –ø—Ä–æ—Å–∏–ª –æ—Ç—Å—Ä–æ—á–∏—Ç—å –µ–≥–æ –≤–æ–∑–≤—Ä–∞—Ç –≤ —Å–≤—è–∑–∏ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–º–∏ —Ç—Ä—É–¥–Ω–æ—Å—Ç—è–º–∏.
        """
        
        print("\nüìã –§–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –û–ë–°–¢–û–Ø–¢–ï–õ–¨–°–¢–í–ê –î–ï–õ–ê:")
        print("-" * 80)
        print(facts.strip())
        print("-" * 80)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ—Ç–∏–≤–∏—Ä–æ–≤–æ—á–Ω–æ–π —á–∞—Å—Ç–∏
        print("\nü§ñ –ì–ï–ù–ï–†–ê–¶–ò–Ø –ú–û–¢–ò–í–ò–†–û–í–û–ß–ù–û–ô –ß–ê–°–¢–ò...")
        print("‚è≥ –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–æ–∂–¥–∏—Ç–µ...")
        
        reasoning = generate(
            model=model,
            tokenizer=tokenizer,
            facts=facts,
            max_input_length=1024,
            max_output_length=1024,
            num_beams=4,
            do_sample=True,
            temperature=0.7,
            top_p=0.9
        )
        
        print("\n‚úÖ –ú–û–¢–ò–í–ò–†–û–í–û–ß–ù–ê–Ø –ß–ê–°–¢–¨ –°–ì–ï–ù–ï–†–ò–†–û–í–ê–ù–ê:")
        print("=" * 80)
        print(reasoning)
        print("=" * 80)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        output_file = "demo_result.txt"
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("–§–ê–ö–¢–ò–ß–ï–°–ö–ò–ï –û–ë–°–¢–û–Ø–¢–ï–õ–¨–°–¢–í–ê –î–ï–õ–ê:\n")
            f.write("-" * 80 + "\n")
            f.write(facts.strip())
            f.write("\n\n" + "=" * 80 + "\n")
            f.write("–ú–û–¢–ò–í–ò–†–û–í–û–ß–ù–ê–Ø –ß–ê–°–¢–¨:\n")
            f.write("-" * 80 + "\n")
            f.write(reasoning)
        
        print(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ —Ñ–∞–π–ª: {output_file}")
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–∞–±–æ—Ç–µ —Å –º–æ–¥–µ–ª—å—é: {str(e)}")
        logger.error(f"–û—à–∏–±–∫–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏: {str(e)}")

def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏
    """
    print("üöÄ –ó–∞–ø—É—Å–∫ –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ —Å–∏—Å—Ç–µ–º—ã LLM-Lawyer")
    print("–ú–æ–¥–µ–ª—å: QVikhr (4.02B –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)")
    print("–°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è: –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –º–æ—Ç–∏–≤–∏—Ä–æ–≤–æ—á–Ω–æ–π —á–∞—Å—Ç–∏ —Å—É–¥–µ–±–Ω—ã—Ö —Ä–µ—à–µ–Ω–∏–π")
    print()
    
    demo_qvikhr()
    
    print("\n" + "="*80)
    print("–î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ó–ê–í–ï–†–®–ï–ù–ê")
    print("="*80)
    print("\n–î–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è —Å–∏—Å—Ç–µ–º—ã:")
    print("1. –û–±—É—á–µ–Ω–∏–µ: python src/train.py --train_file data/train_dataset.jsonl --test_file data/train_dataset_test.jsonl")
    print("2. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è: python src/inference.py --input_file input.txt --output_file output.txt")
    print("3. –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ: python src/test_example.py --test_file data/train_dataset_test.jsonl")
    print("4. GUI: python gui/app.py")

if __name__ == '__main__':
    main() 