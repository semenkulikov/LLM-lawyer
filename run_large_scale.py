#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import argparse
import subprocess
import time
import json
from pathlib import Path
from loguru import logger

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
def load_env():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞"""
    env_file = Path('.env')
    if env_file.exists():
        logger.info("üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞...")
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏–∑ –∑–Ω–∞—á–µ–Ω–∏—è
                    value = value.strip().strip('"').strip("'")
                    os.environ[key.strip()] = value
        logger.success("‚úÖ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    else:
        logger.warning("‚ö†Ô∏è  –§–∞–π–ª .env –Ω–µ –Ω–∞–π–¥–µ–Ω")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
load_env()

def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤"""
    logger.remove()
    logger.add(
        sys.stdout,
        format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
        level="INFO"
    )
    logger.add(
        "logs/large_scale.log",
        rotation="100 MB",
        retention="30 days",
        format="{time:YYYY-MM-DD HH:mm:ss} | {level: <8} | {name}:{function}:{line} - {message}",
        level="DEBUG"
    )

def check_system_requirements():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤"""
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π...")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ RAM
    import psutil
    ram_gb = psutil.virtual_memory().total / (1024**3)
    if ram_gb < 16:
        logger.warning(f"‚ö†Ô∏è  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 16GB RAM, –¥–æ—Å—Ç—É–ø–Ω–æ: {ram_gb:.1f}GB")
    else:
        logger.info(f"‚úÖ RAM: {ram_gb:.1f}GB - OK")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–∏—Å–∫–æ–≤–æ–≥–æ –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–∞
    disk_usage = psutil.disk_usage('.')
    disk_gb = disk_usage.free / (1024**3)
    if disk_gb < 50:
        logger.warning(f"‚ö†Ô∏è  –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –º–∏–Ω–∏–º—É–º 50GB —Å–≤–æ–±–æ–¥–Ω–æ–≥–æ –º–µ—Å—Ç–∞, –¥–æ—Å—Ç—É–ø–Ω–æ: {disk_gb:.1f}GB")
    else:
        logger.info(f"‚úÖ –°–≤–æ–±–æ–¥–Ω–æ–µ –º–µ—Å—Ç–æ: {disk_gb:.1f}GB - OK")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
    try:
        import torch
        if torch.cuda.is_available():
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            logger.info(f"‚úÖ GPU: {torch.cuda.get_device_name(0)} ({gpu_memory:.1f}GB)")
        else:
            logger.warning("‚ö†Ô∏è  CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU (–æ—á–µ–Ω—å –º–µ–¥–ª–µ–Ω–Ω–æ –¥–ª—è 50k+ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤)")
    except ImportError:
        logger.error("‚ùå PyTorch –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

def count_documents(input_dir):
    """–ü–æ–¥—Å—á–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏"""
    pdf_files = list(Path(input_dir).glob("*.pdf"))
    return len(pdf_files)

def estimate_processing_time(num_docs):
    """–û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    # –ü—Ä–∏–º–µ—Ä–Ω—ã–µ –æ—Ü–µ–Ω–∫–∏ (–≤ —á–∞—Å–∞—Ö)
    preprocessing_time = num_docs * 0.001  # 3.6 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
    analysis_time = num_docs * 0.01       # 36 —Å–µ–∫—É–Ω–¥ –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç (OpenAI API)
    training_time = num_docs * 0.0001     # 0.36 —Å–µ–∫—É–Ω–¥—ã –Ω–∞ –¥–æ–∫—É–º–µ–Ω—Ç
    
    total_time = preprocessing_time + analysis_time + training_time
    
    logger.info(f"üìä –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–ª—è {num_docs:,} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤:")
    logger.info(f"   –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞: {preprocessing_time:.1f} —á–∞—Å–æ–≤")
    logger.info(f"   –ê–Ω–∞–ª–∏–∑ OpenAI: {analysis_time:.1f} —á–∞—Å–æ–≤")
    logger.info(f"   –û–±—É—á–µ–Ω–∏–µ: {training_time:.1f} —á–∞—Å–æ–≤")
    logger.info(f"   –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.1f} —á–∞—Å–æ–≤ ({total_time/24:.1f} –¥–Ω–µ–π)")

def create_batch_processing_script(num_docs, batch_size=100):
    """–°–æ–∑–¥–∞–Ω–∏–µ —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    script_content = f"""#!/usr/bin/env python
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–æ–∑–¥–∞–Ω–Ω—ã–π —Å–∫—Ä–∏–ø—Ç –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏ {num_docs:,} –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

import os
import sys
import subprocess
from pathlib import Path

def process_batch(start_idx, end_idx, batch_num):
    print(f"üîÑ –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞ {batch_num}: –¥–æ–∫—É–º–µ–Ω—Ç—ã {start_idx:,}-{end_idx:,}")
    
    # –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
    cmd = [
        "python", "src/preprocess.py",
        "--input-dir", "data/raw",
        "--output-dir", f"data/processed/batch_{batch_num}",
        "--start-index", str(start_idx),
        "--end-index", str(end_idx)
    ]
    subprocess.run(cmd, check=True)
    
    # –ê–Ω–∞–ª–∏–∑ –±–∞—Ç—á–∞
    cmd = [
        "python", "src/process_with_openai.py",
        "--input-dir", f"data/processed/batch_{batch_num}",
        "--output-dir", f"data/analyzed/batch_{batch_num}",
        "--max-workers", "3"
    ]
    subprocess.run(cmd, check=True)

def main():
    total_docs = {num_docs}
    batch_size = {batch_size}
    
    for i in range(0, total_docs, batch_size):
        batch_num = i // batch_size + 1
        start_idx = i
        end_idx = min(i + batch_size, total_docs)
        
        try:
            process_batch(start_idx, end_idx, batch_num)
            print(f"‚úÖ –ë–∞—Ç—á {batch_num} –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –≤ –±–∞—Ç—á–µ {batch_num}: {{e}}")
            break

if __name__ == "__main__":
    main()
"""
    
    with open("process_batches.py", "w", encoding="utf-8") as f:
        f.write(script_content)
    
    logger.info("üìù –°–æ–∑–¥–∞–Ω —Å–∫—Ä–∏–ø—Ç process_batches.py –¥–ª—è –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏")

def run_large_scale_pipeline(args):
    """–ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö"""
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞ –¥–ª—è –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    input_dir = Path(args.input_dir)
    if not input_dir.exists():
        logger.error(f"‚ùå –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {input_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return False
    
    num_docs = count_documents(input_dir)
    logger.info(f"üìÑ –ù–∞–π–¥–µ–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {num_docs:,}")
    
    if num_docs == 0:
        logger.error("‚ùå –î–æ–∫—É–º–µ–Ω—Ç—ã –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return False
    
    # –û—Ü–µ–Ω–∫–∞ –≤—Ä–µ–º–µ–Ω–∏
    estimate_processing_time(num_docs)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –ø–∞–∫–µ—Ç–Ω–æ–≥–æ —Å–∫—Ä–∏–ø—Ç–∞
    create_batch_processing_script(num_docs, args.batch_size)
    
    # –ó–∞–ø—É—Å–∫ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    if args.mode == "batch":
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ –ø–∞–∫–µ—Ç–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏...")
        cmd = ["python", "process_batches.py"]
        subprocess.run(cmd, check=True)
    else:
        logger.info("üîÑ –ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞...")
        cmd = [
            "python", "run_pipeline.py",
            "--epochs", str(args.epochs),
            "--batch-size", str(args.batch_size),
            "--max-docs", str(num_docs)
        ]
        subprocess.run(cmd, check=True)
    
    logger.info("‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
    return True

def main():
    parser = argparse.ArgumentParser(description='–ü–∞–π–ø–ª–∞–π–Ω –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ 50,000+ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤')
    parser.add_argument('--input-dir', type=str, default='data/raw', help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å PDF –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏')
    parser.add_argument('--batch-size', type=int, default=1000, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    parser.add_argument('--epochs', type=int, default=50, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è')
    parser.add_argument('--mode', choices=['batch', 'full'], default='batch', help='–†–µ–∂–∏–º –æ–±—Ä–∞–±–æ—Ç–∫–∏')
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    setup_logging()
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏–π
    check_system_requirements()
    
    # –ó–∞–ø—É—Å–∫ –ø–∞–π–ø–ª–∞–π–Ω–∞
    success = run_large_scale_pipeline(args)
    
    if success:
        logger.info("üéâ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —É—Å–ø–µ—à–Ω–æ!")
    else:
        logger.error("‚ùå –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ —Å –æ—à–∏–±–∫–∞–º–∏")
        sys.exit(1)

if __name__ == "__main__":
    main() 