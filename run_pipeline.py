#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import sys
import argparse
import subprocess
import time
from pathlib import Path
from loguru import logger

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
def load_env():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞"""
    env_file = Path('.env')
    if env_file.exists():
        logger.info("üìÑ –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞...")
        with open(env_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    # –£–±–∏—Ä–∞–µ–º –∫–∞–≤—ã—á–∫–∏ –∏–∑ –∑–Ω–∞—á–µ–Ω–∏—è
                    value = value.strip().strip('"').strip("'")
                    os.environ[key.strip()] = value
        logger.success("‚úÖ –ü–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω—ã")
    else:
        logger.warning("‚ö†Ô∏è  –§–∞–π–ª .env –Ω–µ –Ω–∞–π–¥–µ–Ω")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ
load_env()

def run_command(cmd, description):
    """–ó–∞–ø—É—Å–∫ –∫–æ–º–∞–Ω–¥—ã —Å –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    logger.info(f"üöÄ –ó–∞–ø—É—Å–∫: {description}")
    logger.info(f"üíª –ö–æ–º–∞–Ω–¥–∞: {' '.join(cmd)}")
    logger.info(f"‚è±Ô∏è  –ù–∞—á–∞–ª–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {time.strftime('%H:%M:%S')}")
    
    try:
        start_time = time.time()
        result = subprocess.run(cmd, check=True, capture_output=True, text=True, encoding='utf-8', errors='replace')
        elapsed_time = time.time() - start_time
        
        logger.info(f"‚úÖ {description} –∑–∞–≤–µ—Ä—à–µ–Ω–æ —É—Å–ø–µ—à–Ω–æ –∑–∞ {elapsed_time:.2f} —Å–µ–∫—É–Ω–¥")
        if result.stdout:
            # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –≤—ã–≤–æ–¥–∞
            lines = result.stdout.strip().split('\n')
            if len(lines) > 10:
                logger.info(f"üìÑ –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å—Ç—Ä–æ–∫–∏ –≤—ã–≤–æ–¥–∞:")
                for line in lines[-10:]:
                    logger.info(f"   {line}")
            else:
                logger.info(f"üìÑ –í—ã–≤–æ–¥: {result.stdout.strip()}")
        return True
    except subprocess.CalledProcessError as e:
        logger.error(f"‚ùå –û—à–∏–±–∫–∞ –≤ {description}: {e}")
        if e.stdout:
            logger.error(f"üìÑ stdout: {e.stdout}")
        if e.stderr:
            logger.error(f"üìÑ stderr: {e.stderr}")
        return False

def check_dependencies():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
    logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π...")
    
    required_packages = [
        'torch', 'transformers', 'datasets', 'openai', 
        'pymupdf', 'nltk', 'tqdm', 'numpy'
    ]
    
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    
    if missing_packages:
        logger.error(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç –ø–∞–∫–µ—Ç—ã: {', '.join(missing_packages)}")
        logger.info("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –∏—Ö –∫–æ–º–∞–Ω–¥–æ–π: pip install -r requirements.txt")
        return False
    
    logger.info("‚úì –í—Å–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã")
    return True

def check_openai_key():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è OpenAI API –∫–ª—é—á–∞"""
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        logger.error("–ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω–∞—è –æ–∫—Ä—É–∂–µ–Ω–∏—è OPENAI_API_KEY")
        logger.info("–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –µ—ë –∫–æ–º–∞–Ω–¥–æ–π: set OPENAI_API_KEY=your_key_here")
        return False
    
    logger.info("‚úì OpenAI API –∫–ª—é—á –Ω–∞–π–¥–µ–Ω")
    return True

def create_directories():
    """–°–æ–∑–¥–∞–Ω–∏–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π"""
    directories = [
        'data/raw',
        'data/processed', 
        'data/analyzed',
        'data/structured',
        'models'
    ]
    
    for directory in directories:
        Path(directory).mkdir(parents=True, exist_ok=True)
    
    logger.info("‚úì –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ —Å–æ–∑–¥–∞–Ω—ã")

def step1_preprocess(input_dir=None, output_dir="data/processed"):
    """–®–∞–≥ 1: –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤"""
    if not input_dir:
        input_dir = "data/raw"
    
    if not Path(input_dir).exists():
        logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {input_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫—É.")
        return True
    
    cmd = [
        sys.executable, "src/preprocess.py",
        "--input-dir", input_dir,
        "--output-dir", output_dir
    ]
    
    return run_command(cmd, "–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤")

def step2_analyze_with_openai(input_dir="data/processed", output_dir="data/analyzed", max_docs=50000, max_workers=10):
    """–®–∞–≥ 2: –ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å OpenAI (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π)"""
    if not Path(input_dir).exists():
        logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {input_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑.")
        return True
    
    cmd = [
        sys.executable, "src/process_with_openai.py",
        "--input-dir", input_dir,
        "--output-dir", output_dir,
        "--max-docs", str(max_docs),
        "--max-workers", str(max_workers)
    ]
    
    return run_command(cmd, "–ê–Ω–∞–ª–∏–∑ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å OpenAI")

def step3_build_dataset(analyzed_dir="data/analyzed", output_file="data/train_dataset.jsonl"):
    """–®–∞–≥ 3: –°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    if not Path(analyzed_dir).exists():
        logger.warning(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è {analyzed_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Å–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞.")
        return True
    
    cmd = [
        sys.executable, "src/build_dataset.py",
        "--analyzed-dir", analyzed_dir,
        "--output-file", output_file
    ]
    
    return run_command(cmd, "–°–æ–∑–¥–∞–Ω–∏–µ –æ–±—É—á–∞—é—â–µ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞")

def step4_train_model(train_file="data/train_dataset.jsonl", 
                     test_file="data/train_dataset_test.jsonl",
                     output_dir="models/legal_model",
                     epochs=50,
                     batch_size=8):
    """–®–∞–≥ 4: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ)"""
    if not Path(train_file).exists():
        logger.warning(f"–§–∞–π–ª {train_file} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ.")
        return True
    
    cmd = [
        sys.executable, "src/train.py",
        "--train_file", train_file,
        "--test_file", test_file,
        "--output_dir", output_dir,
        "--epochs", str(epochs),
        "--batch_size", str(batch_size),
        "--learning_rate", "1e-5",  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è Vinthroy
        "--warmup_steps", "50",     # –£–º–µ–Ω—å—à–µ–Ω–æ –¥–ª—è Vinthroy
        "--gradient_accumulation_steps", "8"  # –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è Vinthroy
    ]
    
    return run_command(cmd, "–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ)")

def step5_test_model(model_path="models/legal_model", 
                    test_file="data/train_dataset_test.jsonl",
                    output_dir="results"):
    """–®–∞–≥ 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏"""
    if not Path(model_path).exists():
        logger.warning(f"–ú–æ–¥–µ–ª—å {model_path} –Ω–µ –Ω–∞–π–¥–µ–Ω–∞. –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ.")
        return True
    
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    cmd = [
        sys.executable, "src/test_example.py",
        "--model_path", model_path,
        "--test_file", test_file,
        "--output_dir", output_dir
    ]
    
    return run_command(cmd, "–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏")

def run_full_pipeline(args):
    """–ó–∞–ø—É—Å–∫ –ø–æ–ª–Ω–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
    start_time = time.time()
    
    logger.info("=" * 80)
    logger.info("üöÄ –ó–ê–ü–£–°–ö –ü–û–õ–ù–û–ì–û –ü–ê–ô–ü–õ–ê–ô–ù–ê –û–ë–†–ê–ë–û–¢–ö–ò –î–û–ö–£–ú–ï–ù–¢–û–í")
    logger.info("=" * 80)
    logger.info(f"üìÅ –í—Ö–æ–¥–Ω–∞—è –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {args.input_dir or 'data/raw'}")
    logger.info(f"üéØ –ú–∞–∫—Å–∏–º—É–º –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {args.max_docs}")
    logger.info(f"üß† –≠–ø–æ—Ö–∏ –æ–±—É—á–µ–Ω–∏—è: {args.epochs}")
    logger.info(f"üì¶ –†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞: {args.batch_size}")
    logger.info("=" * 80)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∏
    logger.info("üîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º—ã...")
    if not check_dependencies():
        return False
    
    if not check_openai_key():
        return False
    
    create_directories()
    
    # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ —à–∞–≥–æ–≤
    steps = [
        ("üìÑ –ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ PDF", lambda: step1_preprocess(args.input_dir)),
        ("ü§ñ –ê–Ω–∞–ª–∏–∑ —Å OpenAI", lambda: step2_analyze_with_openai(max_docs=args.max_docs, max_workers=args.max_workers)),
        ("üìä –°–æ–∑–¥–∞–Ω–∏–µ –¥–∞—Ç–∞—Å–µ—Ç–∞", step3_build_dataset),
        ("üß† –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏", lambda: step4_train_model(epochs=args.epochs, batch_size=args.batch_size)),
        ("‚úÖ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏", step5_test_model)
    ]
    
    completed_steps = 0
    for i, (step_name, step_func) in enumerate(steps, 1):
        logger.info(f"\n{'='*60}")
        logger.info(f"üìã –®–∞–≥ {i}/{len(steps)}: {step_name}")
        logger.info(f"{'='*60}")
        
        step_start = time.time()
        if not step_func():
            logger.error(f"‚ùå –ü–∞–π–ø–ª–∞–π–Ω –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –Ω–∞ —à–∞–≥–µ: {step_name}")
            return False
        
        step_time = time.time() - step_start
        completed_steps += 1
        logger.info(f"‚úÖ –®–∞–≥ {i} –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {step_time:.2f} —Å–µ–∫—É–Ω–¥")
    
    total_time = time.time() - start_time
    
    logger.info("\n" + "=" * 80)
    logger.info("üéâ –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù –£–°–ü–ï–®–ù–û!")
    logger.info("=" * 80)
    logger.info(f"üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    logger.info(f"   ‚úÖ –í—ã–ø–æ–ª–Ω–µ–Ω–æ —à–∞–≥–æ–≤: {completed_steps}/{len(steps)}")
    logger.info(f"   ‚è±Ô∏è  –û–±—â–µ–µ –≤—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫—É–Ω–¥ ({total_time/60:.1f} –º–∏–Ω—É—Ç)")
    logger.info(f"   üöÄ –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –Ω–∞ —à–∞–≥: {total_time/len(steps):.2f} —Å–µ–∫—É–Ω–¥")
    logger.info("=" * 80)
    logger.info("üéØ –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
    logger.info("   üì± –î–ª—è –∑–∞–ø—É—Å–∫–∞ GUI: python gui/app.py")
    logger.info("   üß™ –î–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: python demo.py")
    logger.info("   üìä –î–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞: python src/monitor_training.py")
    logger.info("=" * 80)
    
    return True

def main():
    parser = argparse.ArgumentParser(description='–ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω –æ–±—Ä–∞–±–æ—Ç–∫–∏ —é—Ä–∏–¥–∏—á–µ—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤')
    parser.add_argument('--input-dir', type=str, help='–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è —Å –∏—Å—Ö–æ–¥–Ω—ã–º–∏ PDF —Ñ–∞–π–ª–∞–º–∏')
    parser.add_argument('--max-docs', type=int, default=50000, help='–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('--max-workers', type=int, default=10, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã—Ö –ø–æ—Ç–æ–∫–æ–≤ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('--epochs', type=int, default=50, help='–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —ç–ø–æ—Ö –æ–±—É—á–µ–Ω–∏—è (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤)')
    parser.add_argument('--batch-size', type=int, default=8, help='–†–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ (–æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è –±–æ–ª—å—à–∏—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤)')
    parser.add_argument('--skip-training', action='store_true', help='–ü—Ä–æ–ø—É—Å—Ç–∏—Ç—å –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏')
    
    args = parser.parse_args()
    
    if args.skip_training:
        # –¢–æ–ª—å–∫–æ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏ –∞–Ω–∞–ª–∏–∑
        logger.info("–ó–∞–ø—É—Å–∫ –±–µ–∑ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏...")
        check_dependencies()
        check_openai_key()
        create_directories()
        
        step1_preprocess(args.input_dir)
        step2_analyze_with_openai(max_docs=args.max_docs, max_workers=args.max_workers)
        step3_build_dataset()
        
        logger.info("–ü—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –î–ª—è –æ–±—É—á–µ–Ω–∏—è –∑–∞–ø—É—Å—Ç–∏—Ç–µ: python src/train.py")
    else:
        # –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω
        run_full_pipeline(args)

if __name__ == '__main__':
    main() 