#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import glob
from pathlib import Path
from loguru import logger

def merge_datasets(input_dir="datasets/hybrid_generated", output_file="datasets/merged_training_dataset.jsonl"):
    """
    –û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –≤ –æ–¥–∏–Ω —Ñ–∞–π–ª –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    
    Args:
        input_dir: –ü–∞–ø–∫–∞ —Å –¥–∞—Ç–∞—Å–µ—Ç–∞–º–∏
        output_file: –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª
    """
    input_path = Path(input_dir)
    output_path = Path(output_file)
    
    if not input_path.exists():
        logger.error(f"–ü–∞–ø–∫–∞ {input_dir} –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
        return False
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # –ò—â–µ–º –≤—Å–µ JSONL —Ñ–∞–π–ª—ã
    jsonl_files = list(input_path.glob("*.jsonl"))
    
    if not jsonl_files:
        logger.warning(f"–í –ø–∞–ø–∫–µ {input_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ JSONL —Ñ–∞–π–ª–æ–≤")
        return False
    
    logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(jsonl_files)} —Ñ–∞–π–ª–æ–≤ –¥–∞—Ç–∞—Å–µ—Ç–∞")
    
    total_examples = 0
    total_input_chars = 0
    total_output_chars = 0
    
    with open(output_path, 'w', encoding='utf-8') as outfile:
        for jsonl_file in jsonl_files:
            logger.info(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é —Ñ–∞–π–ª: {jsonl_file.name}")
            
            try:
                with open(jsonl_file, 'r', encoding='utf-8') as infile:
                    for line_num, line in enumerate(infile, 1):
                        if line.strip():
                            try:
                                record = json.loads(line)
                                
                                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –∑–∞–ø–∏—Å–∏
                                if 'instruction' in record and 'input' in record and 'output' in record:
                                    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                                    outfile.write(line)
                                    total_examples += 1
                                    total_input_chars += len(record.get('input', ''))
                                    total_output_chars += len(record.get('output', ''))
                                else:
                                    logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ –Ω–µ–∫–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –∑–∞–ø–∏—Å—å –≤ {jsonl_file.name}:{line_num}")
                                    
                            except json.JSONDecodeError as e:
                                logger.warning(f"–û—à–∏–±–∫–∞ JSON –≤ {jsonl_file.name}:{line_num}: {e}")
                                continue
                                
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è —Ñ–∞–π–ª–∞ {jsonl_file}: {e}")
                continue
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    avg_input_length = total_input_chars // total_examples if total_examples > 0 else 0
    avg_output_length = total_output_chars // total_examples if total_examples > 0 else 0
    
    logger.info(f"""
üìä –†–µ–∑—É–ª—å—Ç–∞—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤:

üìÅ –í—ã—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª: {output_path}
üìù –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {total_examples}
üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤–≤–æ–¥–∞: {avg_input_length} —Å–∏–º–≤–æ–ª–æ–≤
üìè –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞ –≤—ã–≤–æ–¥–∞: {avg_output_length} —Å–∏–º–≤–æ–ª–æ–≤
üìä –û–±—â–∏–π –æ–±—ä–µ–º –≤–≤–æ–¥–∞: {total_input_chars} —Å–∏–º–≤–æ–ª–æ–≤
üìä –û–±—â–∏–π –æ–±—ä–µ–º –≤—ã–≤–æ–¥–∞: {total_output_chars} —Å–∏–º–≤–æ–ª–æ–≤

‚úÖ –î–∞—Ç–∞—Å–µ—Ç –≥–æ—Ç–æ–≤ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è!
    """)
    
    return True

def create_training_config(output_file="datasets/training_config.json"):
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    config = {
        "dataset_path": "datasets/merged_training_dataset.jsonl",
        "model_name": "Qwen/Qwen2.5-7B-Instruct",
        "training_args": {
            "output_dir": "models/legal_model_v2",
            "num_train_epochs": 3,
            "per_device_train_batch_size": 2,
            "gradient_accumulation_steps": 4,
            "learning_rate": 2e-5,
            "warmup_steps": 100,
            "logging_steps": 10,
            "save_steps": 500,
            "eval_steps": 500,
            "save_total_limit": 2,
            "fp16": True,
            "gradient_checkpointing": True,
            "dataloader_pin_memory": False
        },
        "lora_config": {
            "r": 16,
            "lora_alpha": 32,
            "target_modules": ["q_proj", "v_proj"],
            "lora_dropout": 0.05,
            "bias": "none",
            "task_type": "CAUSAL_LM"
        }
    }
    
    config_path = Path(output_file)
    config_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(config_path, 'w', encoding='utf-8') as f:
        json.dump(config, f, indent=2, ensure_ascii=False)
    
    logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {config_path}")
    return config_path

if __name__ == "__main__":
    # –û–±—ä–µ–¥–∏–Ω—è–µ–º –¥–∞—Ç–∞—Å–µ—Ç—ã
    success = merge_datasets()
    
    if success:
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        create_training_config()
        
        print("\nüéâ –ì–æ—Ç–æ–≤–æ! –¢–µ–ø–µ—Ä—å –º–æ–∂–Ω–æ –æ–±—É—á–∞—Ç—å –º–æ–¥–µ–ª—å:")
        print("python src/train.py --config datasets/training_config.json")
    else:
        print("\n‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–∏ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤")
