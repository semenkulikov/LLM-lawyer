#!/usr/bin/env python
# -*- coding: utf-8 -*-

import json
import re
from pathlib import Path
from typing import List, Dict, Any
from loguru import logger

class DatasetCleaner:
    """–û—á–∏—Å—Ç–∫–∞ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å LoRA"""
    
    def __init__(self):
        # –†–∞–∑–Ω–æ–æ–±—Ä–∞–∑–Ω—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        self.instructions = [
            "–°–æ—Å—Ç–∞–≤—å –∏—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ –ø–æ —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–º –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞–º –¥–µ–ª–∞.",
            "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø–∏—Å–∞–Ω–∏—è —Å–∏—Ç—É–∞—Ü–∏–∏.",
            "–ù–∞–ø–∏—à–∏ –∏—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ, –∏—Å—Ö–æ–¥—è –∏–∑ –ø—Ä–∏–≤–µ–¥—ë–Ω–Ω—ã—Ö —Ñ–∞–∫—Ç–æ–≤.",
            "–°–æ—Å—Ç–∞–≤—å –∏—Å–∫ –ø–æ —É–∫–∞–∑–∞–Ω–Ω—ã–º –æ–±—Å—Ç–æ—è—Ç–µ–ª—å—Å—Ç–≤–∞–º.",
            "–°–æ–∑–¥–∞–π –∏—Å–∫–æ–≤–æ–µ –∑–∞—è–≤–ª–µ–Ω–∏–µ –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –æ–ø–∏—Å–∞–Ω–Ω—ã—Ö —Å–æ–±—ã—Ç–∏–π.",
            "–ü–æ–¥–≥–æ—Ç–æ–≤—å —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–π –¥–æ–∫—É–º–µ–Ω—Ç –ø–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–º —Ñ–∞–∫—Ç–∞–º.",
            "–°–æ—Å—Ç–∞–≤—å –∑–∞—è–≤–ª–µ–Ω–∏–µ –≤ —Å—É–¥ –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–ø–∏—Å–∞–Ω–Ω–æ–π —Å–∏—Ç—É–∞—Ü–∏–∏."
        ]
        
        # –ö–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏—Å–∫–∞
        self.valid_phrases = [
            "–ò–°–ö–û–í–û–ï –ó–ê–Ø–í–õ–ï–ù–ò–ï",
            "–ú–û–¢–ò–í–ò–†–û–í–û–ß–ù–ê–Ø –ß–ê–°–¢–¨", 
            "–†–ï–ó–û–õ–Æ–¢–ò–í–ù–ê–Ø –ß–ê–°–¢–¨",
            "–í –°–£–î",
            "–ò–°–¢–ï–¶:",
            "–û–¢–í–ï–¢–ß–ò–ö:",
            "–ü–†–û–®–£:",
            "–ù–ê –û–°–ù–û–í–ê–ù–ò–ò –ò–ó–õ–û–ñ–ï–ù–ù–û–ì–û"
        ]
        
        # –§—Ä–∞–∑—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –º—É—Å–æ—Ä–∞
        self.invalid_phrases = [
            "–£–≤–∞–∂–∞–µ–º—ã–π –í–ª–∞–¥–∏–º–∏—Ä –í–ª–∞–¥–∏–º–∏—Ä–æ–≤–∏—á",
            "–î–æ—Ä–æ–≥–æ–π –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å",
            "–°–ø–∞—Å–∏–±–æ –∑–∞ –æ–±—Ä–∞—â–µ–Ω–∏–µ",
            "–ù–∞–¥–µ—é—Å—å, —ç—Ç–æ –ø–æ–º–æ–∂–µ—Ç",
            "–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã",
            "–û–±—Ä–∞—Ç–∏—Ç–µ—Å—å –∫ —é—Ä–∏—Å—Ç—É",
            "–≠—Ç–æ –Ω–µ —é—Ä–∏–¥–∏—á–µ—Å–∫–∞—è –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏—è"
        ]
    
    def clean_dataset(self, input_file: str, output_file: str) -> Dict[str, Any]:
        """
        –û—á–∏—Å—Ç–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞
        
        Args:
            input_file: –í—Ö–æ–¥–Ω–æ–π —Ñ–∞–π–ª –¥–∞—Ç–∞—Å–µ—Ç–∞
            output_file: –í—ã—Ö–æ–¥–Ω–æ–π –æ—á–∏—â–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
            
        Returns:
            –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏
        """
        logger.info(f"–ù–∞—á–∏–Ω–∞—é –æ—á–∏—Å—Ç–∫—É –¥–∞—Ç–∞—Å–µ—Ç–∞: {input_file}")
        
        cleaned_examples = []
        total_examples = 0
        filtered_examples = 0
        
        with open(input_file, 'r', encoding='utf-8') as f:
            for line_num, line in enumerate(f, 1):
                if not line.strip():
                    continue
                    
                total_examples += 1
                
                try:
                    example = json.loads(line)
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
                    if not self._validate_structure(example):
                        logger.warning(f"–ü—Ä–æ–ø—É—â–µ–Ω–∞ –∑–∞–ø–∏—Å—å {line_num}: –Ω–µ–≤–µ—Ä–Ω–∞—è —Å—Ç—Ä—É–∫—Ç—É—Ä–∞")
                        filtered_examples += 1
                        continue
                    
                    # –û—á–∏—â–∞–µ–º –ø—Ä–∏–º–µ—Ä
                    cleaned_example = self._clean_example(example)
                    
                    if cleaned_example:
                        cleaned_examples.append(cleaned_example)
                    else:
                        filtered_examples += 1
                        
                except json.JSONDecodeError as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ JSON –≤ —Å—Ç—Ä–æ–∫–µ {line_num}: {e}")
                    filtered_examples += 1
                    continue
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º –æ—á–∏—â–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
        self._save_cleaned_dataset(cleaned_examples, output_file)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = {
            'total_examples': total_examples,
            'cleaned_examples': len(cleaned_examples),
            'filtered_examples': filtered_examples,
            'cleaning_rate': len(cleaned_examples) / total_examples if total_examples > 0 else 0
        }
        
        logger.info(f"""
üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞:

üìù –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {stats['total_examples']}
‚úÖ –û—á–∏—â–µ–Ω–æ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {stats['cleaned_examples']}
‚ùå –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ: {stats['filtered_examples']}
üìà –ü—Ä–æ—Ü–µ–Ω—Ç –æ—á–∏—Å—Ç–∫–∏: {stats['cleaning_rate']:.1%}

üìÅ –û—á–∏—â–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç: {output_file}
        """)
        
        return stats
    
    def _validate_structure(self, example: Dict[str, Any]) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –ø—Ä–∏–º–µ—Ä–∞"""
        required_fields = ['instruction', 'input', 'output']
        return all(field in example for field in required_fields)
    
    def _clean_example(self, example: Dict[str, Any]) -> Dict[str, Any]:
        """–û—á–∏—Å—Ç–∫–∞ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞"""
        output = example.get('output', '').strip()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å –∏—Å–∫–∞
        if not self._is_valid_legal_document(output):
            return None
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –º—É—Å–æ—Ä
        if self._contains_garbage(output):
            return None
        
        # –û—á–∏—â–∞–µ–º —Ç–µ–∫—Å—Ç
        cleaned_output = self._clean_text(output)
        
        if not cleaned_output or len(cleaned_output) < 100:
            return None
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–µ–π
        cleaned_example = {
            'instruction': self._get_random_instruction(),
            'input': example.get('input', '').strip(),
            'output': cleaned_output
        }
        
        # –î–æ–±–∞–≤–ª—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –µ—Å–ª–∏ –µ—Å—Ç—å
        if 'metadata' in example:
            cleaned_example['metadata'] = example['metadata']
        
        return cleaned_example
    
    def _is_valid_legal_document(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –≤–∞–ª–∏–¥–Ω–æ—Å—Ç—å —é—Ä–∏–¥–∏—á–µ—Å–∫–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
        text_upper = text.upper()
        
        # –î–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ö–æ—Ç—è –±—ã 2 –∫–ª—é—á–µ–≤—ã–µ —Ñ—Ä–∞–∑—ã
        valid_count = sum(1 for phrase in self.valid_phrases if phrase in text_upper)
        
        # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞
        if len(text) < 200:
            return False
        
        # –î–æ–ª–∂–Ω–∞ —Å–æ–¥–µ—Ä–∂–∞—Ç—å –º–æ—Ç–∏–≤–∏—Ä–æ–≤–æ—á–Ω—É—é —á–∞—Å—Ç—å
        if "–ú–û–¢–ò–í–ò–†–û–í–û–ß–ù–ê–Ø –ß–ê–°–¢–¨" not in text_upper:
            return False
        
        return valid_count >= 2
    
    def _contains_garbage(self, text: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–∞–ª–∏—á–∏–µ –º—É—Å–æ—Ä–∞"""
        text_lower = text.lower()
        
        for phrase in self.invalid_phrases:
            if phrase.lower() in text_lower:
                return True
        
        return False
    
    def _clean_text(self, text: str) -> str:
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –ª–∏—à–Ω–∏—Ö —ç–ª–µ–º–µ–Ω—Ç–æ–≤"""
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –ø—Ä–æ–±–µ–ª—ã –∏ –ø–µ—Ä–µ–Ω–æ—Å—ã
        text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
        text = re.sub(r' +', ' ', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã –≤ –Ω–∞—á–∞–ª–µ –∏ –∫–æ–Ω—Ü–µ
        text = text.strip()
        
        # –£–±–∏—Ä–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–∏ –≤ –∫–≤–∞–¥—Ä–∞—Ç–Ω—ã—Ö —Å–∫–æ–±–∫–∞—Ö
        text = re.sub(r'\[.*?\]', '', text)
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ –∫–∞–≤—ã—á–∫–∏
        text = re.sub(r'[""]', '"', text)
        
        return text.strip()
    
    def _get_random_instruction(self) -> str:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ª—É—á–∞–π–Ω–æ–π –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏"""
        import random
        return random.choice(self.instructions)
    
    def _save_cleaned_dataset(self, examples: List[Dict[str, Any]], output_file: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—á–∏—â–µ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
        output_path = Path(output_file)
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(output_file, 'w', encoding='utf-8') as f:
            for example in examples:
                f.write(json.dumps(example, ensure_ascii=False) + '\n')
        
        logger.info(f"–û—á–∏—â–µ–Ω–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_file}")

def create_lora_training_config(dataset_path: str, output_dir: str = "models/legal_model_lora") -> Dict[str, Any]:
    """–°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è —Å LoRA"""
    
    config = {
        "dataset_path": dataset_path,
        "model_name": "Vikhrmodels/QVikhr-3-4B-Instruction",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≤–∞—à—É –º–æ–¥–µ–ª—å
        "training_args": {
            "output_dir": output_dir,
            "num_train_epochs": 3,
            "per_device_train_batch_size": 1,  # –ú–∞–ª–µ–Ω—å–∫–∏–π –±–∞—Ç—á –¥–ª—è 8GB GPU
            "gradient_accumulation_steps": 8,
            "learning_rate": 1e-4,  # –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è LoRA
            "warmup_steps": 50,
            "logging_steps": 10,
            "save_steps": 200,
            "eval_steps": 200,
            "save_total_limit": 2,
            "fp16": True,
            "gradient_checkpointing": True,
            "dataloader_pin_memory": False,
            "remove_unused_columns": False,
            "load_best_model_at_end": True,
            "metric_for_best_model": "eval_loss",
            "greater_is_better": False
        },
        "lora_config": {
            "r": 16,
            "lora_alpha": 32,
            "target_modules": ["q_proj", "v_proj", "k_proj", "o_proj"],
            "lora_dropout": 0.05,
            "bias": "none",
            "task_type": "CAUSAL_LM"
        },
        "data_config": {
            "max_length": 2048,
            "truncation": True,
            "padding": "max_length"
        }
    }
    
    return config

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ—á–∏—Å—Ç–∫–∏ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    input_file = "datasets/merged_training_dataset.jsonl"
    output_file = "datasets/clean_training_dataset.jsonl"
    
    if not Path(input_file).exists():
        logger.error(f"–§–∞–π–ª {input_file} –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return
    
    # –û—á–∏—â–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    cleaner = DatasetCleaner()
    stats = cleaner.clean_dataset(input_file, output_file)
    
    if stats['cleaned_examples'] > 0:
        # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –¥–ª—è LoRA
        config = create_lora_training_config(output_file)
        
        config_file = "datasets/lora_training_config.json"
        with open(config_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        logger.info(f"""
üéâ –û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞! 

üìä –†–µ–∑—É–ª—å—Ç–∞—Ç:
- –û—á–∏—â–µ–Ω–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {stats['cleaned_examples']}
- –ü—Ä–æ—Ü–µ–Ω—Ç –æ—á–∏—Å—Ç–∫–∏: {stats['cleaning_rate']:.1%}

üöÄ –î–ª—è –æ–±—É—á–µ–Ω–∏—è —Å LoRA –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ:
python src/train_lora.py --config {config_file}

üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:
- –ï—Å–ª–∏ –ø—Ä–∏–º–µ—Ä–æ–≤ < 1000, —Å–æ–±–µ—Ä–∏—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö
- –î–ª—è –ª—É—á—à–µ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ 3-5 —ç–ø–æ—Ö
- –ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ eval_loss –¥–ª—è –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–µ–Ω–∏—è –ø–µ—Ä–µ–æ–±—É—á–µ–Ω–∏—è
        """)
    else:
        logger.error("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –æ—á–∏—Å—Ç–∏—Ç—å –Ω–∏ –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞!")

if __name__ == "__main__":
    main()
