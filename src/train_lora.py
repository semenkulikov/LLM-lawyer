#!/usr/bin/env python
# -*- coding: utf-8 -*-

import os
import json
import argparse
from pathlib import Path
from typing import Dict, Any

import torch
from transformers import (
    AutoTokenizer, AutoModelForCausalLM, 
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import Dataset
from loguru import logger

def load_config(config_path: str) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
    with open(config_path, 'r', encoding='utf-8') as f:
        return json.load(f)

def load_dataset(dataset_path: str) -> Dataset:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –¥–∞—Ç–∞—Å–µ—Ç: {dataset_path}")
    
    examples = []
    with open(dataset_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():
                examples.append(json.loads(line))
    
    logger.info(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(examples)} –ø—Ä–∏–º–µ—Ä–æ–≤")
    return Dataset.from_list(examples)

def format_prompt(example: Dict[str, str]) -> str:
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"""
    instruction = example.get('instruction', '')
    input_text = example.get('input', '')
    output = example.get('output', '')
    
    # –§–æ—Ä–º–∞—Ç –¥–ª—è –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è
    prompt = f"### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏—è:\n{instruction}\n\n### –í—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\n{input_text}\n\n### –û—Ç–≤–µ—Ç:\n{output}"
    
    return prompt

def tokenize_function(examples: Dict[str, Any], tokenizer, max_length: int = 2048) -> Dict[str, Any]:
    """–¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö"""
    prompts = [format_prompt(ex) for ex in examples]
    
    tokenized = tokenizer(
        prompts,
        truncation=True,
        padding="max_length",
        max_length=max_length,
        return_tensors="pt"
    )
    
    # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º labels —Ä–∞–≤–Ω—ã–º–∏ input_ids –¥–ª—è causal LM
    tokenized["labels"] = tokenized["input_ids"].clone()
    
    return tokenized

def setup_lora_config(config: Dict[str, Any]) -> LoraConfig:
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ LoRA"""
    lora_config = config.get('lora_config', {})
    
    return LoraConfig(
        r=lora_config.get('r', 16),
        lora_alpha=lora_config.get('lora_alpha', 32),
        target_modules=lora_config.get('target_modules', ["q_proj", "v_proj"]),
        lora_dropout=lora_config.get('lora_dropout', 0.05),
        bias=lora_config.get('bias', "none"),
        task_type=TaskType.CAUSAL_LM
    )

def train_model(config_path: str):
    """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å LoRA"""
    logger.info("üöÄ –ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å LoRA")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    config = load_config(config_path)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º CUDA
    device = "cuda" if torch.cuda.is_available() else "cpu"
    logger.info(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}")
    
    if device == "cuda":
        logger.info(f"GPU: {torch.cuda.get_device_name()}")
        logger.info(f"–ü–∞–º—è—Ç—å GPU: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä
    model_name = config['model_name']
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä: {model_name}")
    
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    
    # –î–æ–±–∞–≤–ª—è–µ–º pad_token –µ—Å–ª–∏ –µ–≥–æ –Ω–µ—Ç
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    logger.info(f"–ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å: {model_name}")
    
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16 if device == "cuda" else torch.float32,
        device_map="auto" if device == "cuda" else None,
        trust_remote_code=True
    )
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º LoRA
    logger.info("–ù–∞—Å—Ç—Ä–∞–∏–≤–∞—é LoRA")
    lora_config = setup_lora_config(config)
    model = get_peft_model(model, lora_config)
    
    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞—Ö
    model.print_trainable_parameters()
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    dataset = load_dataset(config['dataset_path'])
    
    # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º –¥–∞—Ç–∞—Å–µ—Ç
    logger.info("–¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É—é –¥–∞—Ç–∞—Å–µ—Ç")
    data_config = config.get('data_config', {})
    max_length = data_config.get('max_length', 2048)
    
    tokenized_dataset = dataset.map(
        lambda x: tokenize_function(x, tokenizer, max_length),
        batched=True,
        remove_columns=dataset.column_names
    )
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –æ–±—É—á–µ–Ω–∏—è
    training_args = TrainingArguments(
        **config['training_args'],
        report_to=None,  # –û—Ç–∫–ª—é—á–∞–µ–º wandb
        dataloader_pin_memory=False if device == "cpu" else True
    )
    
    # Data collator
    data_collator = DataCollatorForLanguageModeling(
        tokenizer=tokenizer,
        mlm=False
    )
    
    # –°–æ–∑–¥–∞–µ–º —Ç—Ä–µ–Ω–µ—Ä
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_dataset,
        data_collator=data_collator,
        tokenizer=tokenizer
    )
    
    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª—å
    logger.info("–ù–∞—á–∏–Ω–∞—é –æ–±—É—á–µ–Ω–∏–µ...")
    trainer.train()
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–æ–¥–µ–ª—å
    output_dir = config['training_args']['output_dir']
    logger.info(f"–°–æ—Ö—Ä–∞–Ω—è—é –º–æ–¥–µ–ª—å –≤: {output_dir}")
    
    trainer.save_model()
    tokenizer.save_pretrained(output_dir)
    
    logger.info("‚úÖ –û–±—É—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
    logger.info(f"–ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {output_dir}")

def main():
    parser = argparse.ArgumentParser(description="–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å LoRA")
    parser.add_argument("--config", type=str, required=True, help="–ü—É—Ç—å –∫ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω–æ–º—É —Ñ–∞–π–ª—É")
    
    args = parser.parse_args()
    
    if not os.path.exists(args.config):
        logger.error(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {args.config}")
        return
    
    try:
        train_model(args.config)
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏: {e}")
        raise

if __name__ == "__main__":
    main()
